# Teeny-Orb Experiment-Driven Development Plan v3.0

## Overview

This document outlines an experiment-driven approach to building teeny-orb, focusing on Model Context Protocol (MCP) as the primary foundation. Each phase tests specific hypotheses through weekly experiments, with results documented in lab reports for public learning.

**Development Philosophy:**
- **Build in public** with weekly lab reports documenting successes and failures
- **Experiment-driven** development where each phase tests specific hypotheses  
- **Learning-first** approach that values understanding over feature completion
- **MCP-focused** foundation with adaptive architecture based on findings

---

## 📊 Current Development Status

**Last Updated**: 2025-06-21

### Experiment Phase Overview

| Phase | Status | Progress | Key Hypotheses | Experiment Focus |
|-------|--------|----------|----------------|------------------|
| **Phase 0** | ✅ | 85% | Foundation preserved | CLI/container infrastructure |
| **Phase 1** | 🔄 | 35% | MCP provides sufficient value through standardization | MCP Foundation (Weeks 1-4) |
| **Phase 2** | ⏳ | 0% | 80% of tasks need 10% of context through intelligent selection | Context Optimization (Weeks 5-8) |
| **Phase 3** | ⏳ | 0% | Semantic sync reduces tokens by 90% vs full transfers | Sync Optimization (Weeks 9-10) |
| **Phase 4** | ⏳ | 0% | Specialized agents outperform monolithic by 3x | Multi-Agent Architecture (Weeks 11-14) |

### Test Coverage Status

- **Total Tests**: 41 passing
- **Unit Tests**: ✅ Comprehensive coverage for CLI and container packages
- **Integration Tests**: 🔄 Docker tests skipped (require daemon)
- **Benchmarks**: ✅ Available for ID generation and session management
- **Coverage Range**: 17.6% - 90.9% (varies by package)

### Build Status

- **Build**: ✅ `go build ./cmd/teeny-orb` succeeds
- **Tests**: ✅ `make test` passes (41/41)
- **Linting**: ❓ Not yet configured
- **Security**: ❓ Not yet configured

### Dependencies Status

| Dependency | Status | Version | Purpose |
|------------|--------|---------|---------|
| `github.com/spf13/cobra` | ✅ | v1.9.1 | CLI framework |
| `github.com/spf13/viper` | ✅ | v1.20.1 | Configuration management |
| `github.com/docker/docker` | ✅ | v28.2.2 | Container operations |
| `golangci-lint` | ❌ | - | Code linting (planned) |
| `gosec` | ❌ | - | Security scanning (planned) |

### Architecture Implementation Status

#### ✅ Completed Components

1. **Session Interface Design**
   - `Session` interface with Docker and host implementations
   - `Manager` interface for session lifecycle
   - `ManagerRegistry` for unified session access

2. **Container Management**
   - Docker session creation and management
   - Host session execution
   - Resource limits and cleanup

3. **CLI Framework**
   - Root command with global flags
   - Subcommands: generate, review, session
   - Cobra/Viper integration

4. **Testing Infrastructure**
   - Mock implementations for testing
   - Interface-based dependency injection
   - Comprehensive unit test coverage

#### 🔄 In Progress Components

1. **Configuration System**
   - Basic Viper integration ✅
   - Config file loading ✅
   - Validation logic ❌

2. **Session Management**
   - Basic session operations ✅
   - File synchronization ✅
   - Volume mounts ❌
   - Session persistence ❌

3. **Development Tools**
   - Makefile with common tasks ✅
   - Linting configuration ❌
   - Security scanning ❌

#### ⏳ Pending Components

1. **LLM Integration** (Phase 3)
2. **MCP Implementation** (Phase 4)  
3. **Context Management** (Phase 5)
4. **Interactive Experience** (Phase 6)
5. **TUI Enhancement** (Phase 7)
6. **Production Features** (Phase 8)

### Next Immediate Actions

1. **Complete Phase 1 Week 1** ✅:
   - ~~Create experiment infrastructure~~ ✅
   - ~~Set up lab report generation system~~ ✅
   - ~~Complete Week 1 MCP vs Direct comparison~~ ✅

2. **Continue Phase 1 (Week 2-4)**:
   - Test interoperability with Claude Desktop
   - Build OpenAI bridge implementation
   - Test security boundaries and permissions
   - Explore advanced MCP features

3. **Phase 1 Findings So Far**:
   - Direct implementation is 3x simpler (150 vs 450 LOC)
   - MCP adds significant performance overhead (80,000x latency increase)
   - MCP provides better standardization and tooling
   - Need to test cross-provider benefits to validate hypothesis

### Known Issues & Technical Debt

- Docker integration tests require daemon (currently skipped)
- Session persistence not implemented
- No configuration validation
- Missing structured logging
- No health checking for containers

---

## Experiment Framework Infrastructure

### Core Components

```
teeny-orb/
├── experiments/
│   ├── framework/              # Reusable experiment infrastructure
│   │   ├── metrics.go         # Measurement collection
│   │   ├── report.go          # Lab report generation
│   │   └── benchmark.go       # Performance testing
│   ├── week1-mcp-baseline/    # MCP vs Direct comparison
│   ├── week2-interop/         # Multi-provider testing
│   └── data/                  # Collected metrics
├── docs/
│   └── lab-reports/           # Weekly findings
└── internal/
    ├── mcp/                   # MCP server implementation
    ├── providers/
    │   ├── direct/            # Direct tool calling
    │   └── bridge/            # MCP-to-provider bridges
    └── config/
        └── experiment.go      # Experiment configuration
```

### Lab Report Structure
Each week produces a comprehensive lab report:
1. **Hypothesis**: Clear prediction with reasoning
2. **Method**: Detailed experimental approach
3. **Results**: Quantitative measurements  
4. **Observations**: Qualitative findings
5. **Failures**: What didn't work and why
6. **Conclusions**: What we learned
7. **Next Questions**: What to explore next

---

## Phase 0: Foundation (Preserve Existing Work)

### Objectives
- Establish development environment
- Set up project foundation
- Configure CI/CD pipeline

### Tasks
- [x] Initialize Go module: `go mod init github.com/rcliao/teeny-orb` ✅
- [x] Set up basic project structure ✅
- [x] Configure `.gitignore`, `.editorconfig` ✅
- [ ] Set up GitHub Actions workflow
- [ ] Configure pre-commit hooks (gofmt, govet, golint)
- [ ] Install and configure security scanners (gosec, nancy)
- [x] Create initial README with vision statement ✅
- [x] Set up Makefile with common tasks ✅

### Dependencies to Install
```bash
go get -u github.com/spf13/cobra@latest ✅
go get -u github.com/spf13/viper@latest ✅
go get -u github.com/docker/docker@latest ✅
go get -u github.com/golangci/golangci-lint/cmd/golangci-lint@latest
```

### Validation Criteria
- [x] `make build` produces binary ✅
- [x] `make test` runs successfully ✅
- [ ] `make lint` passes without errors
- [ ] GitHub Actions pipeline runs on PR

---

## Phase 1: MCP Foundation & Experiments (Weeks 1-4)

### Core Hypothesis
**"Model Context Protocol provides sufficient value through standardization to justify its complexity over direct tool calling"**

### Success Criteria
- Interoperability with 2+ AI interfaces (Claude Desktop, custom)
- Implementation overhead <3x compared to direct approach
- Performance penalty <200ms per operation
- Token overhead <20% increase

### Weekly Experiments

#### Week 1: MCP vs Direct Implementation ✅
**Experiment**: Compare implementation complexity and performance

**Tasks**:
- [x] Implement direct tool calling (baseline) ✅
- [x] Implement MCP tool calling (comparison) ✅  
- [x] Create performance benchmarks ✅
- [x] Measure implementation complexity ✅
- [x] Generate lab report: "The True Cost of Standardization" ✅

**Key Measurements**:
- Lines of code ratio
- Implementation time
- Performance overhead
 - Error handling complexity

#### Week 2: Interoperability Testing
**Experiment**: Test MCP across different AI providers

**Tasks**:
- [ ] Integrate with Claude Desktop
- [ ] Build OpenAI bridge implementation
- [ ] Create cross-provider test suite
- [ ] Generate lab report: "Write Once, Run Everywhere?"

**Key Measurements**:
- Integration success rate
- Protocol compatibility issues
- Performance variations
- Setup complexity

#### Week 3: Security & Permissions
**Experiment**: Evaluate MCP's security model

**Tasks**:
- [ ] Implement MCP permission system
- [ ] Test security boundaries
- [ ] Simulate attack scenarios
- [ ] Generate lab report: "Trusting AI with Your Codebase"

**Key Measurements**:
- Permission granularity
- Security overhead
- Vulnerability discoveries
- User experience impact

#### Week 4: Advanced MCP Features
**Experiment**: Explore stateful operations and tool discovery

**Tasks**:
- [ ] Implement stateful tool operations
- [ ] Build dynamic tool registration
- [ ] Test resource management
- [ ] Generate lab report: "Beyond Basic Tools"

**Decision Point**: Continue with MCP or pivot based on findings

---

## Phase 2: Context Optimization Experiments (Weeks 5-8)

### Core Hypothesis
**"80% of coding tasks can be completed with 10% of project context through intelligent selection"**

### Success Criteria
- Demonstrate 80/10 ratio across 20+ real tasks
- Reduce token usage by >70% vs naive approach
- Maintain task success rate >90%
- Identify patterns in context selection

#### Week 5-6: Context Measurement
**Experiment**: Quantify context requirements

**Tasks**:
- [ ] Analyze task success vs context size
- [ ] Build context selection strategies
- [ ] Profile token usage patterns
- [ ] Generate lab report: "The Context Goldilocks Zone"

#### Week 7-8: Intelligent Selection
**Experiment**: Beat random selection by 50%

**Tasks**:
- [ ] Implement smart context algorithms
- [ ] Run performance comparisons
- [ ] Analyze cost implications
- [ ] Generate lab report: "Teaching AI to Focus"

---

## Phase 3: Sync Optimization Experiments (Weeks 9-10)

### Core Hypothesis
**"Semantic file synchronization can reduce token usage by 90% compared to full file transfers"**

### Success Criteria
- Achieve 90% token reduction for common operations
- Maintain full fidelity of changes
- Keep sync latency <100ms
- Handle edge cases gracefully

**Tasks**:
- [ ] Implement semantic diff system
- [ ] Create performance benchmarks
- [ ] Test edge case handling
- [ ] Generate lab report: "The 90% Solution"

---

## Phase 4: Multi-Agent Architecture Experiments (Weeks 11-14)

### Core Hypothesis
**"Specialized agents with focused context outperform monolithic agents with full context by 3x"**

### Success Criteria
- Demonstrate 3x improvement in task completion speed
- Reduce total token usage by >60%
- Enable parallel task execution
- Maintain or improve output quality

**Tasks**:
- [ ] Build agent specialization framework
- [ ] Create orchestration layer
- [ ] Run performance comparisons
- [ ] Generate lab report: "The Specialist Revolution"

---

## Legacy Planning: CLI Foundation & Configuration

### Objectives
- Build robust CLI infrastructure
- Implement configuration management
- Establish logging and error handling patterns

### Tasks

#### Week 2: CLI Structure
- [x] Implement root command with global flags ✅
- [ ] Add version command with build info
- [ ] Create config command for management
- [x] Implement help system with examples ✅
- [ ] Add shell completion generation
- [ ] Create command aliases for common operations

#### Week 3: Configuration & Logging
- [x] Implement Viper configuration loading ✅
- [x] Support config file formats (YAML, JSON, ENV) ✅  
- [ ] Add configuration validation
- [ ] Implement structured logging with slog
- [ ] Create error types and handling patterns
- [x] Add debug mode with verbose logging ✅

### Code Structure
```go
// internal/cli/root.go
type GlobalOptions struct {
    ConfigFile string
    LogLevel   string
    Debug      bool
}

// internal/config/config.go
type Config struct {
    Providers  ProvidersConfig  `mapstructure:"providers"`
    Container  ContainerConfig  `mapstructure:"container"`
    MCP        MCPConfig        `mapstructure:"mcp"`
    Context    ContextConfig    `mapstructure:"context"`
}
```

### Testing Requirements
- [x] Unit tests for command parsing ✅
- [x] Configuration loading tests ✅
- [ ] Error handling test cases
- [ ] Integration test for CLI workflow

### Validation Criteria
- [x] `teeny-orb --help` shows comprehensive help ✅
- [ ] `teeny-orb config validate` checks configuration
- [ ] Logs are structured and queryable
- [ ] Errors provide actionable information

### Blog Post Outline: "Building Production-Ready CLI Tools in Go"
1. Why Cobra and Viper are the Go standards
2. Structuring commands for discoverability
3. Configuration management best practices
4. Error handling that helps users
5. Testing strategies for CLI applications

---

## Legacy Planning: Container Infrastructure

### Objectives
- Implement secure container management
- Build session-based lifecycle management
- Create file synchronization system

### Tasks

#### Week 4: Docker Integration
- [x] Implement Docker client wrapper ✅
- [x] Create secure container configurations ✅
- [x] Add container lifecycle management ✅
- [x] Implement resource limit enforcement ✅
- [ ] Build container health checking
- [x] Add automatic cleanup mechanisms ✅

#### Week 5: Session Management
- [x] Design session abstraction ✅
- [x] Implement session-to-container mapping ✅
- [x] Create file synchronization logic ✅
- [ ] Add volume mount management
- [ ] Implement session persistence
- [x] Build garbage collection system ✅

### Security Implementation
```go
// internal/container/security.go
func SecureContainerConfig() *container.Config {
    return &container.Config{
        User:        "1000:1000",
        WorkingDir:  "/workspace",
        Env:         minimalEnv(),
        AttachStdin: true,
        OpenStdin:   true,
        SecurityOpt: []string{
            "no-new-privileges:true",
            "seccomp=default",
        },
    }
}
```

### Testing Requirements
- [x] Container creation/destruction tests ✅
- [x] Resource limit verification ✅
- [ ] Security configuration tests
- [x] File sync accuracy tests ✅
- [ ] Session recovery tests

### Validation Criteria
- [ ] Containers start in <3 seconds
- [ ] File changes sync bidirectionally
- [ ] Resource limits are enforced
- [ ] Containers are cleaned up properly
- [ ] Security scan shows no vulnerabilities

---

## Legacy Planning: LLM Provider System

### Objectives
- Build flexible provider architecture
- Implement major LLM providers
- Add middleware for reliability

### Tasks

#### Week 6: Provider Architecture
- [ ] Define LLMProvider interface
- [ ] Implement provider factory
- [ ] Create provider configuration
- [ ] Build OpenAI provider
- [ ] Build Anthropic provider
- [ ] Create mock provider for testing

#### Week 7: Middleware & Features
- [ ] Implement retry middleware
- [ ] Add rate limiting
- [ ] Build request/response logging
- [ ] Implement streaming support
- [ ] Add token counting
- [ ] Create provider health checks

### Provider Interface
```go
// internal/providers/interface.go
type LLMProvider interface {
    Chat(ctx context.Context, req ChatRequest) (*ChatResponse, error)
    ChatStream(ctx context.Context, req ChatRequest) (<-chan StreamChunk, error)
    CountTokens(text string) (int, error)
    GetModel() ModelInfo
}
```

### Testing Requirements
- [ ] Provider interface compliance tests
- [ ] Mock provider behavior tests
- [ ] Retry logic verification
- [ ] Streaming functionality tests
- [ ] Error handling scenarios

### Validation Criteria
- [ ] All providers implement interface correctly
- [ ] Streaming responses work smoothly
- [ ] Retry logic handles transient failures
- [ ] Token counting is accurate
- [ ] Provider switching is seamless

---

## Legacy Planning: MCP Implementation (Preserved for Reference)

### Objectives
- Implement MCP server using mcp-go
- Create secure file operation tools
- Build code analysis capabilities

### Tasks

#### Week 8: MCP Server Setup
- [ ] Integrate mcp-go library
- [ ] Configure MCP server
- [ ] Implement transport layer
- [ ] Add recovery middleware
- [ ] Create tool registry
- [ ] Build resource templates

#### Week 9: Tool Implementation
- [ ] Implement read_file tool
- [ ] Implement write_file tool
- [ ] Implement list_files tool
- [ ] Add code analysis tools
- [ ] Create execution tools
- [ ] Implement security validation

### MCP Tool Example
```go
// internal/mcp/tools/filesystem.go
func RegisterFileSystemTools(s *server.MCPServer) {
    s.AddTool(
        mcp.NewTool("read_file",
            mcp.WithDescription("Read file contents"),
            mcp.WithString("path", mcp.Required())),
        handleReadFile,
    )
}

func handleReadFile(args map[string]interface{}) (*mcp.CallToolResult, error) {
    path := args["path"].(string)
    if err := validatePath(path); err != nil {
        return nil, err
    }
    // Secure file reading implementation
}
```

### Testing Requirements
- [ ] MCP protocol compliance tests
- [ ] Tool execution tests
- [ ] Security boundary tests
- [ ] Error handling verification
- [ ] Integration tests with MCP Inspector

### Validation Criteria
- [ ] MCP Inspector validates protocol
- [ ] All tools execute successfully
- [ ] Security boundaries are enforced
- [ ] Error messages are helpful
- [ ] Performance meets requirements

---

## Legacy Planning: Context Management System (Preserved for Reference)

### Objectives
- Build sophisticated context handling
- Implement multiple retention strategies
- Optimize for performance

### Tasks

#### Week 10: Core Context Management
- [ ] Integrate tiktoken-go
- [ ] Implement sliding window
- [ ] Build priority queue system
- [ ] Create context storage
- [ ] Add token counting cache
- [ ] Implement basic compression

#### Week 11: Advanced Features
- [ ] Build summarization system
- [ ] Implement semantic deduplication
- [ ] Add context persistence
- [ ] Create memory pools
- [ ] Optimize performance
- [ ] Add context analytics

### Context Manager Structure
```go
// internal/context/manager.go
type ContextManager struct {
    maxTokens     int
    window        *SlidingWindow
    priority      *PriorityQueue
    compressor    Compressor
    tokenizer     *TokenizerPool
    cache         *lru.Cache
}
```

### Testing Requirements
- [ ] Token counting accuracy tests
- [ ] Window management tests
- [ ] Priority retention tests
- [ ] Compression effectiveness tests
- [ ] Performance benchmarks
- [ ] Memory usage tests

### Validation Criteria
- [ ] 100k tokens managed in <100ms
- [ ] Context quality maintained
- [ ] Memory usage is bounded
- [ ] Cache hit rate >80%
- [ ] Compression reduces tokens by >30%

---

## Legacy Planning: Interactive Experience (Preserved for Reference)

### Objectives
- Create engaging interactive sessions
- Implement state management
- Add user experience enhancements

### Tasks

#### Week 12: Session Management
- [ ] Design session state model
- [ ] Implement conversation history
- [ ] Add session persistence
- [ ] Create session replay
- [ ] Build command history
- [ ] Implement undo/redo

#### Week 13: UX Enhancements
- [ ] Add progress indicators
- [ ] Implement syntax highlighting
- [ ] Create smart prompts
- [ ] Add auto-completion
- [ ] Build error recovery
- [ ] Create session templates

### Testing Requirements
- [ ] Session state consistency tests
- [ ] History management tests
- [ ] Persistence and recovery tests
- [ ] UX responsiveness tests
- [ ] Error handling tests

### Validation Criteria
- [ ] Sessions persist across restarts
- [ ] History is searchable
- [ ] Progress is clearly indicated
- [ ] Errors don't break flow
- [ ] Response time <200ms for UI

---

## Legacy Planning: TUI Enhancement (Preserved for Reference)

### Objectives
- Build professional TUI with Bubble Tea
- Create intuitive navigation
- Add visual feedback

### Tasks

#### Week 14: TUI Foundation
- [ ] Set up Bubble Tea app structure
- [ ] Create main layout
- [ ] Implement navigation
- [ ] Add viewport management
- [ ] Create component system
- [ ] Build theming support

#### Week 15: Advanced Features
- [ ] Add split panes
- [ ] Implement file browser
- [ ] Create code viewer
- [ ] Add search functionality
- [ ] Build help system
- [ ] Implement keyboard shortcuts

### Testing Requirements
- [ ] Component rendering tests
- [ ] Navigation flow tests
- [ ] Keyboard handling tests
- [ ] Theme switching tests
- [ ] Accessibility tests

### Validation Criteria
- [ ] TUI renders correctly
- [ ] Navigation is intuitive
- [ ] Keyboard shortcuts work
- [ ] Accessibility standards met
- [ ] Performance is smooth

---

## Legacy Planning: Production Readiness (Preserved for Reference)

### Objectives
- Achieve production quality
- Complete documentation
- Prepare for distribution

### Tasks

#### Week 16: Quality Assurance
- [ ] Achieve 80% test coverage
- [ ] Run security audit
- [ ] Performance profiling
- [ ] Memory leak detection
- [ ] Load testing
- [ ] Bug fixing sprint

#### Week 17: Release Preparation
- [ ] Write user documentation
- [ ] Create API documentation
- [ ] Build installation scripts
- [ ] Set up release automation
- [ ] Create demo videos
- [ ] Prepare blog series

### Testing Requirements
- [ ] Full regression test suite
- [ ] Cross-platform testing
- [ ] Security scanning
- [ ] Performance benchmarks
- [ ] User acceptance testing

### Validation Criteria
- [ ] All tests passing
- [ ] No security vulnerabilities
- [ ] Performance targets met
- [ ] Documentation complete
- [ ] Installation works smoothly

---

## Tracking Progress

### Weekly Checklist Template
- [ ] Complete planned tasks
- [ ] Write/update tests
- [ ] Update documentation
- [ ] Run security scans
- [ ] Performance check
- [ ] Blog post progress
- [ ] Team sync/review

### Success Indicators
- **Green**: On track, tests passing
- **Yellow**: Minor delays, some tests failing
- **Red**: Blocked, need help

### Key Metrics to Track
1. Test coverage percentage
2. Build time
3. Binary size
4. Memory usage
5. Response latency
6. Error rate

---

## Blog Series Schedule

1. **Week 3**: "Building Production-Ready CLI Tools in Go"
2. **Week 5**: "Container Security Patterns for Development Tools"
3. **Week 7**: "Designing Pluggable LLM Architectures in Go"
4. **Week 9**: "Implementing Model Context Protocol"
5. **Week 11**: "Managing LLM Context Windows"
6. **Week 13**: "Building Stateful CLI Sessions"
7. **Week 15**: "Creating Delightful TUIs with Bubble Tea"
8. **Week 17**: "Taking a Go CLI Tool to Production"

Each blog post should include:
- Problem statement
- Solution approach
- Code examples
- Lessons learned
- Performance considerations
- Security implications

---

## Risk Management

### Technical Risks & Mitigations
- **Docker API changes**: Pin API version, test compatibility
- **LLM API changes**: Use versioned endpoints, abstract interfaces
- **Memory leaks**: Regular profiling, automated testing
- **Security vulnerabilities**: Automated scanning, regular updates

### Schedule Risks & Mitigations
- **Scope creep**: Strict milestone boundaries
- **Technical debt**: Dedicated cleanup time each phase
- **Learning curve**: Buffer time in estimates
- **External dependencies**: Early integration testing

---

## Next Steps

1. **Immediate Actions**:
   - Set up repository
   - Configure development environment
   - Start Phase 0 tasks

2. **Planning**:
   - Review milestones with stakeholders
   - Set up project tracking
   - Schedule regular reviews

3. **Communication**:
   - Set up blog platform
   - Create project updates template
   - Plan demo schedule

This milestone document serves as your roadmap. Update task status regularly and adjust timelines based on actual progress. Remember: the goal is learning through building, so take time to understand each component deeply.
